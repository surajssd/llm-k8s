apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: deepseek-v2-5
  namespace: default
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
        - name: vllm
          image: ghcr.io/surajssd/llm-k8s/lws-vllm:0.8.3
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - "set -x;
            bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader
            --ray_cluster_size=$LWS_GROUP_SIZE
            --dashboard-host=0.0.0.0
            --metrics-export-port=8080;
            python3 -m vllm.entrypoints.openai.api_server
            --port 8000
            --model deepseek-ai/DeepSeek-V2.5
            --tensor-parallel-size 8
            --pipeline-parallel-size 2
            --enable-prefix-caching
            --max-model-len 8192
            --enforce-eager
            --trust-remote-code"
          env:
          - name: NCCL_DEBUG
            value: INFO
          - name: NCCL_NET_GDR_LEVEL
            value: SYS
          - name: NCCL_IB_DISABLE
            value: "0"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 720
          ports:
          # VLLM port
          - containerPort: 8000
          # Ray dashboard port
          - containerPort: 8265
          # Ray metrics port
          - containerPort: 8080
          # Ray cluster address
          - containerPort: 6379
          resources:
            limits:
              nvidia.com/gpu: "8"
              rdma/shared_ib: 1
            requests:
              nvidia.com/gpu: "8"
              rdma/shared_ib: 1
          securityContext:
            capabilities:
              add: [ "IPC_LOCK" ]
          volumeMounts:
          - name: shm
            mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: leaderworkerset.sigs.k8s.io/name
                  operator: In
                  values:
                  - deepseek-v2-5
              topologyKey: "kubernetes.io/hostname"

    workerTemplate:
      metadata:
        labels:
          role: worker
      spec:
        containers:
        - name: vllm
          image: ghcr.io/surajssd/llm-k8s/lws-vllm:0.8.3
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - "set -x;
            bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker
            --ray_address=$LWS_LEADER_ADDRESS
            --metrics-export-port=8080"
          env:
          - name: NCCL_DEBUG
            value: INFO
          - name: NCCL_NET_GDR_LEVEL
            value: SYS
          - name: NCCL_IB_DISABLE
            value: "0"
          ports:
          # VLLM port
          - containerPort: 8000
          # Ray metrics port
          - containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: "8"
              rdma/shared_ib: 1
            requests:
              nvidia.com/gpu: "8"
              rdma/shared_ib: 1
          securityContext:
            capabilities:
              add: [ "IPC_LOCK" ]
          volumeMounts:
          - name: shm
            mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: leaderworkerset.sigs.k8s.io/name
                  operator: In
                  values:
                  - deepseek-v2-5
              topologyKey: "kubernetes.io/hostname"
