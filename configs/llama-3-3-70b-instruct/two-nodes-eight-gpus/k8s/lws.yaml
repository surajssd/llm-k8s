apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: llama-3-3-70b-instruct
  namespace: default
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
        - name: llama-3-3-70b-instruct-leader
          image: ghcr.io/surajssd/llm-k8s/lws-vllm:0.8.3
          securityContext:
            capabilities:
              add: [ "IPC_LOCK" ]
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - "set -x;
            bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader
            --ray_cluster_size=$LWS_GROUP_SIZE
            --dashboard-host=0.0.0.0
            --metrics-export-port=8080;
            python3 -m vllm.entrypoints.openai.api_server
            --port 8000
            --model meta-llama/Llama-3.3-70B-Instruct
            --tensor-parallel-size 4
            --pipeline-parallel-size 2
            --enable-prefix-caching"
          env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: token
          - name: NCCL_DEBUG
            value: INFO
          - name: NCCL_NET_GDR_LEVEL
            value: SYS
          - name: NCCL_IB_DISABLE
            value: "0"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 720
          ports:
          # VLLM port
          - containerPort: 8000
          # Ray dashboard port
          - containerPort: 8265
          # Ray metrics port
          - containerPort: 8080
          # Ray cluster address
          - containerPort: 6379
          resources:
            limits:
              nvidia.com/gpu: "4"
              rdma/shared_ib: 1
            requests:
              nvidia.com/gpu: "4"
              rdma/shared_ib: 1
          volumeMounts:
          - name: shm
            mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
        # This is done so that the pods don't land on the same machine. We are
        # using 4 GPU per pod but each machine type already has 8 GPUs, so all
        # the pods could land on the same machine.
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: leaderworkerset.sigs.k8s.io/name
                  operator: In
                  values:
                  - llama-3-3-70b-instruct
              topologyKey: "kubernetes.io/hostname"

    workerTemplate:
      metadata:
        labels:
          role: worker
      spec:
        containers:
        - name: llama-3-3-70b-instruct-worker
          image: ghcr.io/surajssd/llm-k8s/lws-vllm:0.8.3
          securityContext:
            capabilities:
              add: [ "IPC_LOCK" ]
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - "set -x;
            bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker
            --ray_address=$LWS_LEADER_ADDRESS
            --metrics-export-port=8080"
          env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: token
          - name: NCCL_DEBUG
            value: INFO
          - name: NCCL_NET_GDR_LEVEL
            value: SYS
          - name: NCCL_IB_DISABLE
            value: "0"
          ports:
          # VLLM port
          - containerPort: 8000
          # Ray metrics port
          - containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: "4"
              rdma/shared_ib: 1
            requests:
              nvidia.com/gpu: "4"
              rdma/shared_ib: 1
          volumeMounts:
          - name: shm
            mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: leaderworkerset.sigs.k8s.io/name
                  operator: In
                  values:
                  - llama-3-3-70b-instruct
              topologyKey: "kubernetes.io/hostname"
