distributed:
  enabled: true

cli: "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader
        --ray_cluster_size=$LWS_GROUP_SIZE
        --dashboard-host=0.0.0.0
        --metrics-export-port=8888;
      python3 -m vllm.entrypoints.openai.api_server
        --port 8000
        --model meta-llama/Llama-3.3-70B-Instruct
        --tensor-parallel-size 4
        --pipeline-parallel-size 2
        --enable-prefix-caching"

resources:
  nvidia.com/gpu: "4"
  # Enable IB.
  rdma/shared_ib: 1

huggingFaceToken: true
