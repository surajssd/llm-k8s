apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: llama-3-3-70b-instruct
  namespace: default
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
        - name: llama-3-3-70b-instruct-leader
          image: ghcr.io/surajssd/llm-k8s/lws-vllm:0.7.3
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - "/vllm-workspace/ray_init.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE) --dashboard-host=0.0.0.0 --metrics-export-port=8080;
            python3 -m vllm.entrypoints.openai.api_server
            --port 8000
            --model meta-llama/Llama-3.3-70B-Instruct
            --tensor-parallel-size 2
            --pipeline-parallel-size 2"
          env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: token
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 720
          ports:
          # VLLM port
          - containerPort: 8000
          # Ray dashboard port
          - containerPort: 8265
          # Ray metrics port
          - containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: "2"
            requests:
              nvidia.com/gpu: "2"
          volumeMounts:
          - name: shm
            mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory

    workerTemplate:
      metadata:
        labels:
          role: worker
      spec:
        containers:
        - name: llama-3-3-70b-instruct-worker
          image: ghcr.io/surajssd/llm-k8s/lws-vllm:0.7.3
          imagePullPolicy: Always
          command:
          - sh
          - -c
          # TODO: Figure out how this can be an infiniband IP?
          - "/vllm-workspace/ray_init.sh worker --ray_address=$(LWS_LEADER_ADDRESS) --metrics-export-port=8080"
          env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: token
          ports:
          # VLLM port
          - containerPort: 8000
          # Ray metrics port
          - containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: "2"
            requests:
              nvidia.com/gpu: "2"
          volumeMounts:
          - name: shm
            mountPath: /dev/shm
        volumes:
        - name: shm
          emptyDir:
            medium: Memory
