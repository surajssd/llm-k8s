single:
  enabled: true
  env:
    LMCACHE_LOCAL_CPU: "True"
    LMCACHE_MAX_LOCAL_CPU_SIZE: "500"
    LMCACHE_USE_EXPERIMENTAL: "True"
    VLLM_RPC_TIMEOUT: "1000000"
    LMCACHE_CHUNK_SIZE: "256"

resources:
  # Model is 32b and bf16
  nvidia.com/gpu: "1"

images:
  # Find more tags here: https://hub.docker.com/r/lmcache/vllm-openai/tags
  vLLM: lmcache/vllm-openai:v0.3.7

# Source: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
cli: "/opt/venv/bin/vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
      --tensor-parallel-size 1
      --max-model-len 25000
      --kv-transfer-config
        '{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_both\"}'"

storage:
  ephemeral:
    enabled: true
    storage: 214748364800 # 200GB
