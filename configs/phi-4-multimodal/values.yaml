name: phi-4-multimodal

single:
  enabled: true

resources:
  # The model needs only 12 GB of GPU memory.
  nvidia.com/gpu: "1"

# Args: https://huggingface.co/microsoft/Phi-4-multimodal-instruct#vllm-inference
cli: "pip install vllm[audio];
        python3 -m vllm.entrypoints.openai.api_server
          --model 'microsoft/Phi-4-multimodal-instruct'
          --tensor-parallel-size 1
          --pipeline-parallel-size 1
          --distributed-executor-backend mp
          --dtype auto
          --trust-remote-code
          --max-model-len 131072
          --enable-lora
          --max-lora-rank 320
          --lora-extra-vocab-size 256
          --limit-mm-per-prompt audio=3,image=3
          --max-loras 2
          --enable-prefix-caching"
